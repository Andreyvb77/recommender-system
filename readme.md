# Эксперименты с RL на Acrobot-v1: A2C против PPO и гамма-сканирование
Данный репозиторий содержит воспроизводимые эксперименты по обучению агента управления этапом маятника (Gymnasium: Acrobot-v1) с использованием Stable-Baselines3.
### Содержание
* Описание экспериментов
* Быстрый старт
* Установка и в зависимости
* Запуск экспериментов
* Предварительный просмотр и проверка документов
* Результаты и комментарии
* Тонкая настройка и советы
* Известные обращения и разговоры
### Описание экспериментов
Эксперимент 1:
Сравнение алгоритмов A2C и PPO в одной среде с прогрессивной архитектурой сети (MLP 64-64) Гипотеза: A2C происходит быстрее и стабильнее на тех же этапах из-за частых обновлений; PPO может передавать тонкую настройку n_steps для стабильности.
Эксперимент 2:
Исследование гаммы в гамме A2C = 0,95 против 0,99 против 0,999 (остальные гиперпараметры постоянны) Гипотеза: γ=0,99 обеспечивает баланс между скоростью сходимости и качеством решений; γ=0,999 требуется большее количество мер для сходимости.
Среда
Гимназия: Acrobot-v1 (дискретные действия, быстрая сходимость на CPU)
### Быстрый старт
В Google Colab
1. Открыть ноутбук
2. Смонтируйте Google Drive (ячейка с монтированием)
3. Запустите последовательно все ячейки
4. Артефакты сохраняются в указанной привязке на Диске
### После запуска вы получите:
* Графики (присуждаемые для каждого запуска и общие)
* Видео финала/лучшего агента (mp4)
* CSV
### Установка и зависимости
Минимальный набор:
* Python 3.10+
* спортзал
* stable-baselines3[extra]
* matplotlib
* панды
* opencv-python (для записей видео)
### Применение:
Параметры по умолчанию:
* total_timesteps = 150_000 при запуске
* Оценка по 20 эпизодам
* SEED = 42 (фиксирован для воспроизводимости)
* n_steps = 5 для A2C, n_steps = 128 для PPO
### Запуск экспериментов
1. Подготовка:
* Смонтируйте Google Диск
* Создаем папок для документов
* Фиксировать семя
2. Учебные модели:
3. Оценка и визуализация:
* Построить графики обучения
* Оцените финальные модели на 20 эпизодах
* Запиши видео лучшего агента
* Сохраните результаты в структурированных файлах.
### Предварительный просмотр и проверка документов
Глобальный график (в корневой документации):
* сравнение.png - кривые средние награды (скользящее среднее) для всех запусков на одном рисунке
График каждого запуска:
* <RL_BASE_DIR>/logs/<RUN_NAME>/run_reward_plot.png Видео конечного агента:
* <RL_VIDEOS_DIR>/rl-video-episode-0.mp Колическая оценка (среднее ± стандартное значение по 20 эпизодам):
* Печатается в консоли по завершении запуска
* Сохраняются в:
    * <RL_RESULTS_DIR>/results.json (final_eval_mean_20, final_eval_std_20)
    * <RL_BASE_DIR>/summary_results.csv (сводная таблица показателей)
### Структура документов
После начала:
<RL_BASE_DIR>/ ├── сравнение.png # общий график по всем запускам ├── summary_results.csv # сводная таблица метрик ├── models/ │ ├── a2c_gamma0.99.zip # сохраняемые модели │ ├── ppo_gamma0.99.zip │ ├── a2c_gamma0.95.zip │ └── a2c_gamma0.999.zip ├── logs/ │ ├── a2c_gamma0.99/ │ │ ├── Monitor.csv # лог эпизодов (обучение) │ │ └── run_reward_plot.png # график средний награды │ ├── ppo_gamma0.99/ │ ├── a2c_gamma0.95/ │ └── a2c_gamma0.999/ ├── videos/ │ └── rl-video-episode-0.mp4 # видео лучшего агента └── results/ └── results.csv # количественные результаты
### Результаты и комментарии
полученные результаты на CPU (150 тыс. шагов, начальное число = 42; ближе к 0 - лучше):
* PPO (γ=0,99) -81,8
* A2C (γ=0,99) -170,8
* A2C (γ=0,95) -108,0
* A2C (γ=0.999) -500.0
### Интерпретация:
1. Сравнение A2C и PPO:
* PPO значительно превзошел A2C при тех же бюджетных показателях (-81,8 против -170,8).
* Обрезанная цель в PPO обеспечивает стабильность обучения даже с разреженной наградой.
* A2C показал чувствительность сердца к гиперпараметрам (требуется тонкая настройка n_steps, ent_coef).
2. Влияние гамма в A2C:
γ=0,95 показало быструю сходимость, но высокая дисперсия (±91,8) помешала принять решение. γ=0,99 не смог справиться с локальным минимумом без дополнительных улучшений. γ=0,999 привело к катастрофическому результату (-500,0) — агент полностью застрял в режиме бесконечного раскачивания.
### Выводы:
Для Acrobot-v1 PPO с γ=0,99 представляет собой схему конфигурации при ограниченном бюджете. Выбор алгоритма критического выбора гиперпараметров — PPO тогда стабильно решает задачу, поскольку A2C требует доработки ресурсов. Высокие значения γ (0,999) ограничения без формирования вознаграждения или обучения по учебной программе.
### Настройка и советы
Для улучшения результатов A2C рекомендуется:
* Добавить формирование вознаграждения (бонус за приближение к цели: +0,1 * (cos(θ1) + cos(θ2)))
* Увеличить энтропийную регуляризацию (ent_coef=0,01)
* Увеличить бюджет шагов до 250 тыс. для γ=0.Использовать более частые обновления (n_steps=3-5)
Для изменения стабильности PPO:
* Уменьшить clip_range до 0,1-0.
* Добавить результаты нормализации (VecNormalize)
* Увеличить размер пакета до 128
### Важные замечания:
* Результаты могут быть получены в зависимости от случайного начального числа (рекомендуется усреднение при 5+ запусках)
* A2C обеспечивает чувствительность головного мозга к гиперпараметрам n_steps и гамма
* Для статистической инновации требуется несколько прогонов с разными исходными данными.
* Результаты, полученные на ЦП в Colab, на других платформах могут быть получены.
* γ=0,999 требует не просто увеличения бюджетных мер, а и фундаментальной перенастройки обучения.
### Рекомендации для продолжения:
* Протестировать изменение PPO + формирование вознаграждения
* Провести автоматический выбор гиперпараметров через Optuna
* Исследовать структуру сети ([32] против [64,64] против [128])
* Увеличить бюджет до 250 тыс. шагов для A2C с γ=0,99
