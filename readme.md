# RL Experiments on Acrobot-v1: A2C vs PPO and Gamma Sweep

Данный репозиторий содержит воспроизводимые эксперименты по обучению агента управления двойным маятником (Gymnasium: Acrobot-v1) с использованием Stable-Baselines3.

### Содержание

- Описание экспериментов
- Быстрый старт
- Установка и зависимости
- Запуск экспериментов
- Визуализация и проверка артефактов
- Результаты и комментарии
- Тонкая настройка и советы
- Известные предупреждения и оговорки

### Описание экспериментов

Эксперимент 1:

Сравнение алгоритмов A2C vs PPO на одной среде, с сопоставимой архитектурой сети (MLP 64-64)
Гипотеза: A2C сойдётся быстрее и стабильнее при тех же шагах из-за частых обновлений; 
PPO может потребовать тонкой настройки n_steps для стабильности.

Эксперимент 2: 

Влияние gamma в A2C gamma = 0.95 vs 0.99 vs 0.999 (остальные гиперпараметры фиксированы)
Гипотеза: γ=0.99 обеспечит оптимальный баланс между скоростью сходимости и качеством решения; 
γ=0.999 потребует большего бюджета шагов для сходимости.

### Среда

Gymnasium: Acrobot-v1 (дискретные действия, быстрая сходимость на CPU)

### Быстрый старт

В Google Colab

1. Откройте ноутбук
2. Смонтируйте Google Drive (ячейка с монтированием)
3. Запустите последовательно все ячейки
4. Артефакты сохранятся в указанную папку на Drive

После запуска вы получите:

- Графики наград (по каждому запуску и общий)
- Видео финального/лучшего агента (mp4)
- CSV 

### Установка и зависимости

Минимальный набор:

- Python 3.10+
- gymnasium
- stable-baselines3[extra]
- matplotlib
- pandas
- opencv-python (для записи видео)

### Установка:

Параметры по умолчанию:

- total_timesteps = 150_000 на запуск
- Оценка по 20 эпизодам
- SEED = 42 (фиксированный для воспроизводимости)
- n_steps = 5 для A2C, n_steps = 128 для PPO

### Запуск экспериментов

1. Подготовка:

- Смонтируйте Google Drive
- Создайте структуру папок для артефактов
- Фиксируйте seed

2. Обучение моделей:
3. Оценка и визуализация:

- Постройте графики обучения
- Оцените финальные модели на 20 эпизодах
- Запишите видео лучшего агента
- Сохраните результаты в структурированные файлы

### Визуализация и проверка артефактов

Глобальный график (в корневой папке артефактов):
- comparison.png - кривые средней награды (скользящее среднее) для всех запусков на одном рисунке

График для каждого запуска:

- <RL_BASE_DIR>/logs/<RUN_NAME>/run_reward_plot.png
Видео финального агента:

- <RL_VIDEOS_DIR>/rl-video-episode-0.mpКоличественная оценка (mean ± std по 20 эпизодам):

- Печатается в консоль при завершении запуска
- Сохраняется в:
    - <RL_RESULTS_DIR>/results.json (final_eval_mean_20, final_eval_std_20)
    - <RL_BASE_DIR>/summary_results.csv (сводная таблица метрик)

### Структура артефактов

После запуска:

<RL_BASE_DIR>/
├── comparison.png                 # общий график по всем запускам
├── summary_results.csv            # сводная таблица метрик
├── models/
│   ├── a2c_gamma0.99.zip          # сохраненные модели
│   ├── ppo_gamma0.99.zip
│   ├── a2c_gamma0.95.zip
│   └── a2c_gamma0.999.zip
├── logs/
│   ├── a2c_gamma0.99/
│   │   ├── monitor.csv            # лог эпизодов (обучение)
│   │   └── run_reward_plot.png    # график средней награды
│   ├── ppo_gamma0.99/
│   ├── a2c_gamma0.95/
│   └── a2c_gamma0.999/
├── videos/
│   └── rl-video-episode-0.mp4     # видео лучшего агента
└── results/
    └── results.csv                # количественные результаты

### Результаты и комментарии

Фактические результаты на CPU (150k шагов, seed=42; ближе к 0 - лучше):

- PPO (γ=0.99) -81.8 
- A2C (γ=0.99) -170.8 
- A2C (γ=0.95) -108.0 
- A2C (γ=0.999) -500.0 

### Интерпретация:

1. Сравнение A2C и PPO:

- PPO значительно превзошел A2C при одинаковом бюджете шагов (-81.8 против -170.8).
- Clipped objective в PPO обеспечил стабильность обучения даже с разреженной наградой.
- A2C показал высокую чувствительность к гиперпараметрам (требуется тонкая настройка n_steps, ent_coef).

2. Влияние gamma в A2C:

γ=0.95 показал попытку быстрой сходимости, но высокая дисперсия (±91.8) помешала решению.
γ=0.99 не смог преодолеть локальный минимум без дополнительных улучшений.
γ=0.999 привел к катастрофическому результату (-500.0) — агент полностью застрял в режиме бесконечного раскачивания.

### Выводы:

Для Acrobot-v1 PPO с γ=0.99 является предпочтительной конфигурацией при ограниченном бюджете.
Выбор алгоритма критичнее выбора гиперпараметров — PPO стабильно решает задачу, тогда как A2C требует значительной доработки.
Высокие значения γ (0.999) неприменимы без reward shaping или curriculum learning.

### Настройка и советы

Для улучшения результатов A2C рекомендуется:

- Добавить reward shaping (бонус за приближение к цели: +0.1 * (cos(θ1) + cos(θ2)))
- Увеличить энтропийную регуляризацию (ent_coef=0.01)
- Увеличить бюджет шагов до 250k для γ=0.Использовать более частые обновления (n_steps=3-5)

Для повышения стабильности PPO:

- Снизить clip_range до 0.1-0.
- Добавить нормализацию наблюдений (VecNormalize)
- Увеличить batch_size до 128

### Важные замечания:

- Результаты могут варьироваться в зависимости от random seed (рекомендуется усреднение по 5+ запускам)
- A2C демонстрирует высокую чувствительность к гиперпараметрам n_steps и gamma
- Для статистической значимости необходимы multiple runs с разными seed
- Результаты получены на CPU в Colab, на других платформах могут отличаться
- γ=0.999 требует не просто увеличения бюджета шагов, но и фундаментальной перенастройки обучения

### Рекомендации для продолжения:

- Протестировать комбинацию PPO + reward shaping
- Провести автоматический подбор гиперпараметров через Optuna
- Исследовать влияние архитектуры сети ([32] vs [64,64] vs [128])
- Увеличить бюджет до 250k шагов для A2C с γ=0.99
